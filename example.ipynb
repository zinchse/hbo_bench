{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oracle import Oracle, OracleRequest\n",
    "from utils import get_cardinalities, get_selectivities, MAX_TREE_LENGTH\n",
    "from vectorization import extract_vertices_and_edges, ALL_OPERATIONS\n",
    "import math\n",
    "import torch\n",
    "from dataset import WeightedBinaryTreeDataset, weighted_binary_tree_collate\n",
    "from torch.utils.data import DataLoader\n",
    "from data_config import DEFAULT_DOP, DEFAULT_HINTSET, HINTSETS, DOPS\n",
    "from query_explorer import QueryExplorer, SearchingSettings, SearchingState\n",
    "from local_search_settings import PRUNED_LOCAL_SS, GREEDY_SS, EMPTY_SS, ALL_SS\n",
    "from collections import deque\n",
    "from utils import get_full_plan, preprocess\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *oracle* in math is some object that allows to get answers to some questions **without any computational cost**. In our case, the oracle will answer for each of the queries (and each set of parameters) on the questions:\n",
    "- *What is the* [`plan` / `planning_time` / `execution_time`] *of that query with applying such parameters?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpch_oracle = Oracle(\"data/processed/tpch_10gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan for query 'q01' with hintset=0 and dop=1 is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'plan': {'node_type': 'Sort',\n",
       "  'plans': [{'node_type': 'Aggregate',\n",
       "    'plans': [{'node_type': 'Seq Scan',\n",
       "      'plans': [],\n",
       "      'estimated_cardinality': 58767934,\n",
       "      'index_name': None,\n",
       "      'relation_name': 'lineitem',\n",
       "      'cost': 2165082.91}],\n",
       "    'estimated_cardinality': 6,\n",
       "    'index_name': None,\n",
       "    'relation_name': None,\n",
       "    'cost': 4515800.38}],\n",
       "  'estimated_cardinality': 6,\n",
       "  'index_name': None,\n",
       "  'relation_name': None,\n",
       "  'cost': 4515800.47},\n",
       " 'template_id': 177413717,\n",
       " 'planning_time': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan = tpch_oracle.get_explain_plan(OracleRequest(query_name=\"q01\", hintset=0, dop=1))\n",
    "print(\"Plan for query 'q01' with hintset=0 and dop=1 is:\")\n",
    "plan.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node cardinality is actually the value of the `estimated_cardinality` field, and `selectivity` is the fraction of tuples that were selected during filtering. It is computed in general as the ratio of the cardinality of the parent to the product of the cardinality of the children. We collect vectors of these statistics by left-first in-order plan traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_cardinalities(plan) == [6, 6, 58767934]\n",
    "# for simplicity we calculate cardinality instead of selectivity for leaf nodes\n",
    "assert get_selectivities(plan) == [1.0, 6/58767934, float(58767934)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to vectorise the tree, we use the following encoding scheme.\n",
    "\n",
    "We collect all 25 operators (occurring in the plans from `JOB` and `sample_queries` benchmarks), and use one-hot encoding to encode them. We then extend it with two values of statistics -- `math.log(cardinality)` and `selectivity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices, edges = extract_vertices_and_edges(plan)\n",
    "time = torch.Tensor([1.0])\n",
    "sort_index = ALL_OPERATIONS.index(\"Sort\")\n",
    "assert vertices.shape[1] == len(ALL_OPERATIONS) + 2\n",
    "assert torch.all(vertices[0][0:sort_index] == 0) and torch.all(vertices[0][sort_index+1:-2] == 0)\n",
    "assert vertices[0][sort_index] == 1.0\n",
    "assert vertices[0][-1] == 1.0\n",
    "assert vertices[0][-2] == math.log(6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "To form a efficient dataset from a vectorised representation of trees, we store **only unique** trees, describing them by the number of repetitions and average execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 42\n",
    "dataset = WeightedBinaryTreeDataset([vertices] * freq, [edges] * freq, [time] * freq, torch.device(\"cpu\"))\n",
    "for v, e, f, t in dataset:\n",
    "    assert torch.all(v - vertices == 0)\n",
    "    assert torch.all(e - edges == 0)\n",
    "    assert torch.all(t - time == 0)\n",
    "    assert torch.all(f - torch.Tensor([freq]) == 0)\n",
    "assert len(dataset) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "The key to creating a right dataloader is that we change the order of the axes. The first channels should be those related to the dimensionality of the vector representation of nodes (so-called *channels*), and the tree is represented by a sequence (of arbitrary length) of such vectors (see `weighted_binary_tree_collate` or `preprocess` for implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 42\n",
    "plan = tpch_oracle.get_explain_plan(OracleRequest(query_name=\"q01\", hintset=0, dop=1))\n",
    "(vertices, edges), time = extract_vertices_and_edges(plan), torch.Tensor([1.0])\n",
    "dataset = WeightedBinaryTreeDataset([vertices] * freq, [edges] * freq, [time] * freq, torch.device(\"cpu\"))\n",
    "assert len(dataset) == 1\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda el: weighted_binary_tree_collate(el, MAX_TREE_LENGTH),\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "vertices, edges = preprocess(vertices, edges)\n",
    "for (v, e, f), t in dataloader:\n",
    "    assert torch.all(v[0] - vertices == 0)\n",
    "    assert torch.all(e[0] - edges == 0)\n",
    "    assert torch.all(t[0] - time == 0)\n",
    "    assert torch.all(f[0] - torch.Tensor([freq]) == 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hint based optimisation\n",
    "\n",
    "The main advantage of data from that project (and the presented functionality over them) is that we can test the feasibility of applying hint based optimisation **without having to actually execute queries**! This allows us to go through **thousands** of different configurations of the search algorithm and quickly test different hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of fast checking the possibilities of hint based optimisation on `TPCH` benchmark.\n",
    "\n",
    "P.S. *Note that this is the **most inconvenient** benchmark for optimisation, because almost all modern RDBMSs are overfitted for it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query | Default Time (s) | Best Time (s) | Best Params (hs, dop)\n",
      "q01 | 131.01 | 129.74 | (0, 1)\n",
      "q02 | 32.22 | 12.72 | (16, 1)\n",
      "q03 | 1.76 | 1.74 | (89, 64)\n",
      "q04 | 1.99 | 1.99 | (8, 64)\n",
      "q05 | 2.80 | 2.80 | (8, 64)\n",
      "q06 | 3.97 | 3.81 | (0, 1)\n",
      "q07 | 11.27 | 10.85 | (33, 1)\n",
      "q08 | 4.52 | 1.23 | (8, 16)\n",
      "q09 | 1.24 | 1.13 | (32, 1)\n",
      "q10 | 54.87 | 49.55 | (35, 1)\n",
      "q11 | 23.95 | 2.30 | (1, 1)\n",
      "q12 | 1.34 | 1.26 | (40, 64)\n",
      "q13 | 76.82 | 25.15 | (96, 16)\n",
      "q14 | 12.91 | 6.88 | (98, 64)\n",
      "q15 | 3.88 | 3.88 | (8, 64)\n",
      "q16 | 20.95 | 20.95 | (32, 64)\n",
      "q17 | 23.80 | 4.74 | (16, 64)\n",
      "q18 | 15.46 | 9.91 | (38, 64)\n",
      "q19 | 4.44 | 4.25 | (0, 1)\n",
      "q20 | 19.91 | 19.77 | (41, 1)\n",
      "q21 | 6.87 | 6.03 | (80, 1)\n",
      "q22 | 58.98 | 55.37 | (16, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Query | Default Time (s) | Best Time (s) | Best Params (hs, dop)\")\n",
    "\n",
    "total_default_time, total_with_best_parameters_time = 0, 0\n",
    "\n",
    "for query_name in sorted(tpch_oracle.get_query_names()):\n",
    "    def_request = OracleRequest(query_name=query_name, hintset=DEFAULT_HINTSET, dop=DEFAULT_DOP)\n",
    "    default_time = tpch_oracle.get_planning_time(def_request) + tpch_oracle.get_execution_time(def_request)\n",
    "    \n",
    "    best_params = min(\n",
    "        ((hintset, dop) for hintset in HINTSETS for dop in DOPS),\n",
    "        key=lambda params: (\n",
    "            tpch_oracle.get_planning_time(OracleRequest(query_name=query_name, hintset=params[0], dop=params[1])) +\n",
    "            tpch_oracle.get_execution_time(OracleRequest(query_name=query_name, hintset=params[0], dop=params[1]))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    with_best_parameters_time = (\n",
    "        tpch_oracle.get_planning_time(OracleRequest(query_name=query_name, hintset=best_params[0], dop=best_params[1])) +\n",
    "        tpch_oracle.get_execution_time(OracleRequest(query_name=query_name, hintset=best_params[0], dop=best_params[1]))\n",
    "    )\n",
    "\n",
    "    print(f\"{query_name} | {default_time / 1000:0.2f} | {with_best_parameters_time/1000:0.2f} | {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that applying hints can significantly speed up queries (`q11`, `q13`, `q17`, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Exploration Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using an exhaustive search algorithm to find the best parameters, we can use **local search algorithms**. The simplest example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emulate_exhaustive_paralell_search(oracle: \"Oracle\", query_name: \"QueryName\", searching_settings: \"SearchingSettings\") -> \"Tuple[SerachingState, Time]\":\n",
    "    \n",
    "    def _prepare_request(state: \"SearchingState\") -> \"OracleRequest\":\n",
    "        return OracleRequest(query_name=query_name, hintset=state.hintset, dop=state.dop)\n",
    "\n",
    "    threads = [0.0 for _ in searching_settings.hardcoded_dops]\n",
    "    \n",
    "    queue = deque(\n",
    "        [SearchingState(hs, dop) for hs in searching_settings.hardcoded_hintsets for dop in searching_settings.hardcoded_dops]\n",
    "    )\n",
    "    seen_plans = set()\n",
    "\n",
    "    def_state = SearchingState(searching_settings.default_hintset, searching_settings.default_dop)\n",
    "    record_state, record_time = def_state, float(\"inf\")\n",
    "    last_time = 0.0\n",
    "    \n",
    "    while queue:\n",
    "        st = deque.popleft(queue)\n",
    "        time = heappop(threads)\n",
    "        plan, plan_time = get_full_plan(oracle.get_explain_plan(_prepare_request(st))), oracle.get_planning_time(_prepare_request(st)) / 1000\n",
    "        time += plan_time\n",
    "        ex_time = oracle.get_execution_time(_prepare_request(st)) / 1000\n",
    "        # optimisations: use timeout and avoiding the re-execution of duplacates\n",
    "        time += min(ex_time, record_time) * (plan not in seen_plans)\n",
    "        seen_plans.add(plan)    \n",
    "        if plan_time + ex_time < record_time:\n",
    "            record_state, record_time = st, plan_time + ex_time\n",
    "        last_time = max(last_time, time)\n",
    "        heappush(threads, time)\n",
    "    \n",
    "    return last_time, record_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_strategies(q_n: \"QueryName\") -> \"None\":\n",
    "    for ss, ss_descr in [\n",
    "        (PRUNED_LOCAL_SS, \"Pruned Local\"),\n",
    "        (GREEDY_SS, \"Greedy\"),\n",
    "    ]:\n",
    "        explorer = QueryExplorer(tpch_oracle, q_n, ss)\n",
    "        def_state, best_state = SearchingState(DEFAULT_HINTSET, DEFAULT_DOP), explorer.run()\n",
    "        print(f\"{q_n}: [{ss_descr}] Learned in {explorer.parallel_e2e_time:0.1f}s; E2E Time {explorer.get_e2e_time(def_state):0.2f}s -> {explorer.get_e2e_time(best_state):0.2f}s\")\n",
    "\n",
    "    explorer = QueryExplorer(tpch_oracle, q_n, EMPTY_SS)\n",
    "    def_state, (learning_time, best_state) = SearchingState(0, 64), emulate_exhaustive_paralell_search(tpch_oracle, q_n, ALL_SS)\n",
    "    print(f\"{q_n}: [Exhaustive]: Learned in {learning_time:0.1f}s; E2E Time {explorer.get_e2e_time(def_state):0.2f}s -> {explorer.get_e2e_time(best_state):0.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q11: [Pruned Local] Learned in 2.3s; E2E Time 23.95s -> 2.30s\n",
      "q11: [Greedy] Learned in 7.2s; E2E Time 23.95s -> 2.30s\n",
      "q11: [Exhaustive]: Learned in 24.3s; E2E Time 23.95s -> 2.30s\n",
      "\n",
      "q13: [Pruned Local] Learned in 25.2s; E2E Time 76.82s -> 25.15s\n",
      "q13: [Greedy] Learned in 101.1s; E2E Time 76.82s -> 25.15s\n",
      "q13: [Exhaustive]: Learned in 672.0s; E2E Time 76.82s -> 25.15s\n",
      "\n",
      "q17: [Pruned Local] Learned in 4.7s; E2E Time 23.80s -> 4.74s\n",
      "q17: [Greedy] Learned in 9.5s; E2E Time 23.80s -> 4.74s\n",
      "q17: [Exhaustive]: Learned in 38.6s; E2E Time 23.80s -> 4.74s\n"
     ]
    }
   ],
   "source": [
    "compare_strategies(\"q11\")\n",
    "print() \n",
    "compare_strategies(\"q13\")\n",
    "print() \n",
    "compare_strategies(\"q17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that exhaustive search allows to find the best combinations of hints, but the time of such search is much longer than other algorithms. Thus, for example, the greedy algorithm allows to achieve almost the same execution time, but spends several times less time on query research.\n",
    "\n",
    "The pruned local search algorithm, on the other hand, allows to learn even faster, with almost no loss in performance. Moreover, if we take into account the cost of obtaining such predictions, the pruned local search algorithm becomes the unquestionable leader."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
